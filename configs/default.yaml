# Default configuration for Battleship RL training

# Environment settings
environment:
  board_size: [4, 4]  # (rows, cols)
  render_mode: null  # Set to "human" or "ansi" for debugging

# Training settings
training:
  total_timesteps: 20000  # Quick prototype: 20K timesteps (~10K episodes)
  eval_freq: 5000  # Evaluate every N timesteps
  save_freq: 5000  # Save checkpoint every N timesteps
  n_eval_episodes: 2  # Number of episodes for evaluation
  verbose: 1  # 0=none, 1=info, 2=debug

# PPO-specific hyperparameters
ppo:
  learning_rate: 0.0003  # 3e-4 is standard for PPO
  n_steps: 2048  # Number of steps per update (increased for faster training)
  batch_size: 128  # Smaller batches = less dilution of rare good episodes, more gradient updates
  n_epochs: 10  # Number of epochs for policy update (reduced for faster training)
  gamma: 0.95  # Discount factor
  gae_lambda: 0.95  # GAE lambda for advantage estimation
  clip_range: 0.2  # PPO clipping parameter
  ent_coef: 0.005  # Reduced exploration - agent has learned basics, now exploit
  vf_coef: 1.0  # Value function coefficient
  max_grad_norm: 1.0  # Gradient clipping
  policy: "MultiInputPolicy"  # Policy type for Dict observation space

  # Network architecture
  policy_kwargs:
    net_arch: [64, 64]
      # - 512  # Hidden layer 1
      # - 512  # Hidden layer 2
      # - 512 # Hidden layer 3
      # - 512  # Hidden layer 4
      # - 256  # Hidden layer 5
      # - 128  # Hidden layer 6
    activation_fn: "relu"  # or " tanh"

# DQN-specific hyperparameters
dqn:
  learning_rate: 0.0001  # 1e-4 is typical for DQN
  buffer_size: 50000  # Replay buffer size
  learning_starts: 1000  # Steps before learning starts
  batch_size: 32  # Batch size for training
  gamma: 0.99  # Discount factor
  tau: 1.0  # Hard update (1.0) or soft update (0.005)
  target_update_interval: 500  # Update target network every N steps
  train_freq: 4  # Train every N steps
  gradient_steps: 1  # Gradient steps per update
  exploration_fraction: 0.3  # Fraction of training for exploration
  exploration_initial_eps: 1.0  # Initial epsilon for exploration
  exploration_final_eps: 0.05  # Final epsilon (5% random actions)
  max_grad_norm: 10  # Gradient clipping
  policy: "MultiInputPolicy"  # Policy type for Dict observation space

  # Network architecture
  policy_kwargs:
    net_arch:
      - 256  # Hidden layer 1
      - 256  # Hidden layer 2
      - 128  # Hidden layer 3
    activation_fn: "relu"

# Logging and experiment tracking
logging:
  use_wandb: true  # Enable Weights & Biases logging
  use_tensorboard: true  # Enable TensorBoard logging
  project_name: "battleship-rl-adjacency"  # W&B project name
  experiment_name: null  # Auto-generated if null
  log_dir: "logs"  # Directory for TensorBoard logs
  save_dir: "models"  # Directory for model checkpoints

# Reward structure (from environment)
# Documented here for reference - changes require environment modification
rewards:
  # Base rewards (before time penalty and escalating penalty)
  miss: -1.0  # Base penalty for misses
  hit: 2.0  # Base reward for hitting a ship
  sink: 5.0  # Base reward for sinking a ship
  win: 5.0  # Base reward for winning (all ships sunk)
  invalid: -50.0  # Penalty for invalid moves (attacking same cell)

  # Bonuses and penalties
  adjacency_bonus: 25.0  # Bonus for attacking adjacent to UNSUNK hits only (teaches target mode)
  missed_adjacency_penalty: -15.0  # Penalty for ignoring adjacency when opportunities exist
  time_penalty: -0.3  # Per-move penalty to encourage efficiency (applied to ALL moves)

  # Escalating penalty (for 5x5 board)
  escalation_threshold: 15  # Start penalizing after this many moves
  escalation_rate: -2.0  # Penalty per move over threshold: -(move_count - threshold) * rate

  # Effective rewards (with time penalty, before escalation or adjacency)
  # miss_effective: -1.3  # -1.0 + (-0.3)
  # hit_effective: 1.7    # 2.0 + (-0.3)
  # sink_effective: 4.7   # 5.0 + (-0.3)
  # win_effective: 4.7    # 5.0 + (-0.3)

# Evaluation
evaluation:
  deterministic: true  # Use deterministic policy for evaluation
  render: false  # Render evaluation episodes
  save_replay: false  # Save replay data

  # Baseline agents for comparison
  baselines:
    - random  # RandomAgent
    - probability  # ProbabilityAgent (DataGenetics algorithm)
